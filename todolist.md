- [ ]  xgboost，rf，GBDT三者的区别，以及带来模型的提升多大，为什么不采用其他方法，是否比较过这三者的应用场景
- [ ] 线性回归与LR的原理与区别，LR的损失函数，以及为什么采用sigmod函数
- [ ]  svm的原理以及对偶问题，为什么采用对偶，解释一下对偶
    - [ ] 手推SVM的loss function
    - [ ] SVM的kernel的选择
    - [ ]  比较logistic回归和SVM
- [ ]  三种决策树的分裂标准，决策树停止生成的条件，如何防止过拟合
- [ ] 过拟合的常见方法
- [ ] boosting和bagging的原理与区别，常见的代表有哪些
- [ ] 讲一下AUC，在什么情况下采用AUC值，以及召回率与准确率，如何选择合适的评价指标
- [ ]  比较logistic回归和SVM
- [ ] PCA及其实现（给定n个D维表述的数据点，问怎么实现）
- [ ] 手写一下决策树中信息增益的公式，说说信息增益代表一个什么意思（数据内的混乱度，也叫作信息熵），某个特征的信息增益对总体信息增益的偏离数值比较大，就把它看做是分类特征（ID3算法）
    - [ ]  他说什么情况下会保证上面信息增益中的H最大，这个没遇到过，手推了一下：当所有的p(xi)相等时，H最大
- [ ]  如何判断决策树及随机森林是过拟合了还是欠拟合(NG对于分类问题的过拟合情况有过讲解)
