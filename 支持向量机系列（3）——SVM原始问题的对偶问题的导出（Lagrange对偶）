SVM原始问题：
\min\limits_{w,b} \frac{1}{2}{||w||}^{2} 。
定义L（w，α）= - -α（ - ），其中α≥0，
当 - 时， - L（w，α）= - ；当某个约束 - 时，令对应的α取+∞，其他的α取0，L（w，α）的极大值即可取+∞。
则SVM原问题（带约束 - 的极小值问题）可表示为 -- L（w，α）（无约束 - ，但有约束α≥0的极小-极大问题。换句话说在α≥0的约束条件下，如果有某个 - ，α可以通过趋于+∞使得 - L（w，α）趋于+∞，从而--L（w，α） 没办法取最小值，即α≥0的约束条件就包含了 - 的约束条件）。
其对偶问题即为极大-极小问题：
--  L（w，α）。
上述过程即为Lagrange对偶法导出的对偶问题。
补充：
1.在原始问题转为极小-极大问题时，为什么不直接解极大-极小问题而要解对偶问题？
因为约束α≥0的存在，极小-极大问题的内层极大问题不方便求解，转为极大-极小问题后内层极小问题无约束，可以通过最优性条件求解。
2.对偶问题中的解（w*，b*，α*）也是原问题的解吗？
强对偶定理告诉我们要满足一定的约束规格（constraint qualifications）/正则条件（regularity conditions）时，对偶问题的解也是原问题的解。参见Karush-Kuhn-Tucker conditions。
常用的约束规格有两个：
线性约束规格：约束函数都是线性的（《数据挖掘中的新方法——支持向量机》）；
Slater条件：对于凸优化问题，存在使得不等式约束严格取不等号的点，称之为严格可行点。（大多数博客文章提到的条件）
Slater条件还有个改良的版本，当某些不等式约束是线性的时候，这些不等式约束不必要求能严格取不等号（Convex Optimization – Boyd and Vandenberghe）。




